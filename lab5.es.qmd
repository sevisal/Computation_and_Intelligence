---
title: "Sesi√≥n de laboratorio 5: Aprendizaje No Supervisado"
format:
    html:
        toc: true
        toc-depth: 5
        toc-location: left
        toc-title: "**Contents**"
---

# Objetivos de la sesi√≥n

::: {.objective-card}
üéØ **Objetivo General:** Explorar los principios y aplicaciones del aprendizaje no supervisado utilizando ejemplos visuales e intuitivos. Aprender c√≥mo el agrupamiento y la reducci√≥n de dimensionalidad pueden revelar estructuras ocultas en conjuntos de datos complejos, particularmente en datos de im√°genes.
:::

![](_resources/images/unsupervised.jpg){.img-right}

A diferencia de la regresi√≥n o la clasificaci√≥n, el **aprendizaje no supervisado** no se basa en etiquetas predefinidas. Su objetivo es descubrir **patrones, grupos o estructuras latentes** en los datos mismos. Esto lo hace especialmente √∫til cuando las etiquetas no est√°n disponibles, son costosas o son subjetivas, como a menudo ocurre en el an√°lisis exploratorio de datos o en la investigaci√≥n cient√≠fica, donde buscamos identificar nuevos fen√≥menos.

Los algoritmos de aprendizaje no supervisado encuentran estructura en datos no etiquetados. En otras palabras, el algoritmo intenta "dar sentido" a los datos por s√≠ mismo. A diferencia del aprendizaje supervisado, no hay una verdad fundamental contra la cual comparar las predicciones.

Dos familias principales de m√©todos de aprendizaje no supervisado son:

- **Clustering:** agrupar muestras similares (por ejemplo, K-Means, clustering jer√°rquico).
- **Reducci√≥n de dimensionalidad:** comprimir datos en un conjunto m√°s peque√±o de variables mientras se preserva la mayor cantidad de informaci√≥n posible (por ejemplo, PCA, t-SNE, UMAP).

Ambas t√©cnicas son complementarias: el clustering nos ayuda a identificar categor√≠as potenciales, mientras que la reducci√≥n de dimensionalidad nos ayuda a visualizar o simplificar datos de alta dimensi√≥n.

Esta sesi√≥n de laboratorio te guiar√° a trav√©s de aplicaciones pr√°cticas de estas t√©cnicas utilizando el conjunto de datos **Cars93** para clustering y el conjunto de datos **Fashion-MNIST** para reducci√≥n de dimensionalidad y clustering de im√°genes. Aprender√°s a preprocesar datos, elegir algoritmos apropiados, evaluar resultados e interpretar hallazgos de manera significativa.


---

# 1. Hacer Clustering con el conjunto de datos Cars93

![](_resources/images/smallcar.gif){.img-right}

El conjunto de datos **Cars93** recopila mediciones y variables descriptivas para 93 modelos de autom√≥viles. Es lo suficientemente peque√±o para la exploraci√≥n interactiva, pero lo suficientemente rico como para ilustrar problemas comunes en el clustering: caracter√≠sticas con diferentes escalas, la necesidad de un preprocesamiento sensato y la interpretaci√≥n de los centros de cl√∫ster aprendidos.

En esta parte del laboratorio nos centraremos √∫nicamente en las siguientes caracter√≠sticas num√©ricas:  
`Price`, `MPG.highway`, `MPG.city`, `Horsepower`, `Fuel.tank.capacity`, `Passengers`, `Weight`, `Length`, y `RPM`.  
Al restringir la atenci√≥n a este subconjunto, buscamos aislar las se√±ales mec√°nicas y relacionadas con el tama√±o que t√≠picamente impulsan el clustering de veh√≠culos. Los experimentos en este laboratorio deben utilizar **s√≥lo** las caracter√≠sticas enumeradas anteriormente.


## 1.1. Preprocesamiento y exploraci√≥n de datos
::: {.objective-card}
üéØ **Objetivo:** Probar tus conocimientos sobre el preprocesamiento de conjuntos de datos.
:::

Carga el conjunto de datos desde el siguiente [enlace](https://raw.githubusercontent.com/selva86/datasets/master/Cars93.csv) e inspecciona sus estad√≠sticas globales: rango, escalas t√≠picas y valores faltantes. Interpreta lo que mide cada caracter√≠stica (unidades y significado sem√°ntico) y considera elecciones de preprocesamiento simples que hagan que el clustering sea significativo.

::: {.question-card}
üìù **Preguntas:**  

1. ¬øCu√°les son los pasos clave de preprocesamiento que tomar√°s para preparar los datos para el clustering?
2. ¬øC√≥mo difieren las caracter√≠sticas en escala y distribuci√≥n? ¬øQu√© caracter√≠sticas podr√≠an dominar los c√°lculos de distancia si no se escalan?
3. ¬øHay valores faltantes o at√≠picos que necesiten ser abordados antes del clustering?
:::

## 1.2. Clustering con K-Means y evaluaci√≥n
::: {.objective-card}
üéØ **Objetivo:** Aprender a aplicar el clustering K-Means al conjunto de datos Cars93 y evaluar los resultados.
:::

![](_resources/images/group.jpg){.img-right}
K-Means es un algoritmo de clustering simple y ampliamente utilizado que particiona los datos en *K* grupos en funci√≥n de la similitud de caracter√≠sticas. K-Means particiona el espacio de caracter√≠sticas en *K* regiones al asignar iterativamente puntos al centroide m√°s cercano y recomputar los centroides como medias de cl√∫ster. Los centroides son las medias aritm√©ticas que resumen cada grupo. K-Means es simple, r√°pido e interpretable, pero depende en gran medida del n√∫mero de cl√∫steres *K* y de la m√©trica de distancia inducida por el preprocesamiento elegido.

Al elegir *K* debes equilibrar dos objetivos: (1) calidad del cl√∫ster, que puede cuantificarse con puntuaciones internas como la **silhouette**, y (2) parsimonia: evitamos muchos cl√∫steres peque√±os que a√±aden poca interpretabilidad. La puntuaci√≥n de silhouette mide qu√© tan bien se ajusta cada punto a su cl√∫ster asignado en comparaci√≥n con otros cl√∫steres; una mayor silhouette indica una separaci√≥n m√°s clara.

Utiliza el widget `K-Means` en Orange para probar varios valores de *K* e inspeccionar los centroides en una tabla.

::: {.question-card}
üìù **Preguntas:**  

1. ¬øC√≥mo cambia la puntuaci√≥n de silhouette a medida que var√≠as *K*? ¬øHay un valor √≥ptimo claro?
2. Examina los perfiles de los centroides para diferentes *K*. ¬øRepresentan tipos de veh√≠culos significativos?
3. ¬øQu√© tan sensibles son los cl√∫steres a tus elecciones de preprocesamiento (por ejemplo, escalado)?
:::

## 1.3. Interpretaci√≥n y visualizaci√≥n de cl√∫steres
::: {.objective-card}
üéØ **Objetivo:** Aprender a interpretar y visualizar los cl√∫steres obtenidos de K-Means.
:::

Una vez que hayas elegido un *K* adecuado y obtenido cl√∫steres, el siguiente paso es interpretar lo que representan. Los perfiles de los centroides se pueden convertir de nuevo a las unidades de caracter√≠sticas originales para comprender las caracter√≠sticas t√≠picas de cada cl√∫ster.

K-Means es no supervisado: encuentra estructura en las caracter√≠sticas de entrada sin ver el tipo de autom√≥vil o las etiquetas de segmento. Para evaluar cu√°n significativos son los grupos descubiertos, puedes compararlos con etiquetas conocidas (por ejemplo, tipo o fabricante) y observar las distribuciones de etiquetas dentro de los cl√∫steres. Para esto, utilizaremos un peque√±o fragmento de Python dentro del widget `Python Script` de Orange. El fragmento a continuaci√≥n visualiza, para cada cl√∫ster, la distribuci√≥n de etiquetas verdaderas (si existe una etiqueta verdadera en la tabla). P√©galo en un widget `Python Script` que reciba la salida de `K-Means` (por lo que `in_data` contiene la meta de `Cluster` asignada y cualquier variable de clase disponible).

```python
# Histogram: true-label distribution per cluster
# Paste this into a Python Script widget connected to the output of K-Means.
# It expects a single Orange input (available as `in_data`).

import matplotlib.pyplot as plt
import numpy as np

# --- LOAD DATA ---
X = in_data.X
y = in_data.Y
class_var = in_data.domain.class_var

# --- CONVERT LABELS TO STRINGS IF POSSIBLE ---
if class_var and class_var.is_discrete and class_var.values:
    labels = np.array([class_var.values[int(v)] for v in y])
else:
    labels = np.array(y).astype(str)

# --- EXTRACT CLUSTERS ---
meta_names = [m.name for m in in_data.domain.metas]
if 'Cluster' in meta_names:
    clusters, _ = in_data.get_column_view('Cluster')
    clusters = np.array(clusters)
    cluster_ids = np.unique(clusters)
    n_clusters = len(cluster_ids)
else:
    clusters = np.zeros(len(X))
    cluster_ids = [0]
    n_clusters = 1

# --- PLOT HISTOGRAMS OF LABEL DISTRIBUTION PER CLUSTER ---
fig, ax = plt.subplots(nrows=1, ncols=n_clusters, figsize=(5 * n_clusters, 4))

# ensure iterable axes even if one cluster
if n_clusters == 1:
    ax = [ax]

for i, cid in enumerate(cluster_ids):
    mask = clusters == cid
    cluster_labels = labels[mask]
    unique_vals, counts = np.unique(cluster_labels, return_counts=True)

    ax[i].bar(unique_vals, counts, color="cornflowerblue", alpha=0.8)
    ax[i].set_title(f"Cluster {cid}")
    ax[i].set_xlabel("True label")
    ax[i].set_ylabel("Count")
    ax[i].grid(which='major', color='gray', alpha=0.4, linestyle='dotted')

    # rotate x-axis tick labels by 45 degrees for readability
    ax[i].set_xticks(range(len(unique_vals)))
    ax[i].set_xticklabels(unique_vals, rotation=45, ha='right')

plt.tight_layout()
plt.show()
```
Este c√≥digo genera un histograma para cada cl√∫ster, mostrando cu√°ntos autom√≥viles de cada etiqueta verdadera est√°n presentes. Esto ayuda a evaluar si los cl√∫steres corresponden a categor√≠as significativas.

::: {.question-card}
üìù **Preguntas:**

1. ¬øLos cl√∫steres corresponden a tipos de veh√≠culos o segmentos conocidos? ¬øQu√© cl√∫steres son los m√°s homog√©neos?
2. ¬øHay cl√∫steres que mezclan muchas etiquetas diferentes? ¬øQu√© podr√≠a indicar esto sobre las caracter√≠sticas utilizadas?
:::

## 1.4. An√°lisis de centroides
::: {.objective-card}
üéØ **Objetivo:** Aprender a interpretar los centroides de K-Means en el espacio de caracter√≠sticas original.
:::

![](_resources/images/escale.png){.img-right}

Los centroides resumen lo que K-Means ha aprendido. Si estandarizaste las caracter√≠sticas antes de la agrupaci√≥n, los centroides se expresan en el espacio escalado. Para interpretarlos en unidades reales (por ejemplo, precio en d√≥lares, peso en libras), debes deshacer la escala. Los widgets de Orange no proporcionan una transformaci√≥n inversa autom√°tica, por lo que reconstruiremos los centroides y los desestandarizaremos en un breve `Python Script` que acepta dos entradas: el `File` original (pre-escalado) y los `centroides de K-Means` (post-escalado). El script a continuaci√≥n asume que utilizaste estandarizaci√≥n (media cero, varianza unitaria).

```python
import numpy as np
import Orange

# --- ACCESS INPUTS ---
if in_datas[0].X.shape[0] > in_datas[1].X.shape[0]:
    orig_data = in_datas[0]        # original data
    centroid_data = in_datas[1]    # output from KMeans or scaled data
elif in_datas[0].X.shape[0] < in_datas[1].X.shape[0]:
    orig_data = in_datas[1]        # original data
    centroid_data = in_datas[0]    # output from KMeans or scaled data
else:
    print('Error! One of your input data should be the centroids!')

# --- COMPUTE MEAN AND STD ---
X_orig = orig_data.X
means = np.nanmean(X_orig, axis=0)
stds = np.nanstd(X_orig, axis=0)

# --- INVERSE TRANSFORM CENTROIDS ---
centroids_scaled = centroid_data.X
centroids_rescaled = centroids_scaled * stds + means

# --- CREATE DOMAIN WITHOUT CLASS VARIABLE ---
attrs = orig_data.domain.attributes  # only features
new_domain = Orange.data.Domain(attrs)  # exclude class_var

# --- CREATE TABLE ---
centroid_table = Orange.data.Table.from_numpy(
    new_domain,
    centroids_rescaled
)
centroid_table.name = "De-standardized Centroids"

out_data = centroid_table
```

Pega esto en un widget de `Python Script` que reciba dos entradas (primero el widget `File` original; segundo los `centroides de K-Means`). Puedes conectar la salida a una `Data Table`, `Heat Map` o `Box Plot` para inspeccionar qu√© caracter√≠sticas son altas o bajas en cada centroide.

::: {.question-card}
üìù **Preguntas:**

1. ¬øQu√© revelan los centroides desestandarizados sobre las caracter√≠sticas t√≠picas de cada cl√∫ster?
2. ¬øQu√© caracter√≠sticas distinguen m√°s claramente los cl√∫steres? ¬øHay patrones sorprendentes?
3. ¬øLos centroides corresponden a tipos de veh√≠culos o segmentos de mercado significativos?
:::

---

# 2. Representaci√≥n de Baja Dimensionalidad y Clustering en un Conjunto de Datos de Im√°genes

En esta segunda parte, trabajaremos con datos de im√°genes, espec√≠ficamente, el conjunto de datos **Fashion-MNIST**, para explorar el clustering y las incrustaciones de baja dimensionalidad.  
A diferencia de los datos tabulares *Cars93*, estos conjuntos de datos tienen miles de caracter√≠sticas (p√≠xeles), por lo que la visualizaci√≥n y la interpretaci√≥n requieren **reducci√≥n de dimensionalidad**.

---

## 2.1 Preprocesamiento del Conjunto de Datos

El conjunto de datos **Fashion-MNIST** contiene im√°genes en escala de grises de 28√ó28 p√≠xeles, cada una correspondiente a una de diez categor√≠as de ropa.

<div style="width:50%; margin:auto;">
| Label | Item        |
|:------|:-------------|
| 0 | Camiseta/top |
| 1 | Pantal√≥n |
| 2 | Jersey |
| 3 | Vestido |
| 4 | Abrigo |
| 5 | Sandalias |
| 6 | Camisa |
| 7 | Zapatillas |
| 8 | Bolso |
| 9 | Bot√≠n |
</div>

Aseg√∫rate de que las etiquetas sean **categ√≥ricas**, no num√©ricas (puedes usar el widget `Edit Domain` para cambiarlas).

Para visualizar algunas muestras del conjunto de datos, conecta el widget `File` a un widget `Python Script` y pega lo siguiente:

```python
import numpy as np
import matplotlib.pyplot as plt

# Get data and labels
X = in_data.X
y = in_data.Y
class_var = in_data.domain.class_var
n_samples, n_features = X.shape

# Shuffle data for visualization
idx = np.arange(n_samples)
np.random.shuffle(idx)
X = X[idx]
y = y[idx]

# Convert numeric labels to strings if available
if class_var.is_discrete and class_var.values:
    labels = np.array([class_var.values[int(v)] for v in y])
else:
    labels = y.astype(int)

# Determine image size
side = int(np.sqrt(n_features))
if side * side != n_features:
    print(f"Warning: {n_features} features not a perfect square ‚Äî forcing 28√ó28")
    side = 28

# Plot random samples
n_show = min(25, n_samples)
plt.figure(figsize=(6,6))
for i in range(n_show):
    plt.subplot(5, 5, i + 1)
    plt.imshow(X[i].reshape(side, side), cmap="gray")
    plt.axis("off")
    plt.title(str(labels[i]))
plt.tight_layout()
plt.show()
```

Este c√≥digo mostrar√° una cuadr√≠cula de 25 im√°genes aleatorias del conjunto de datos, cada una etiquetada con su categor√≠a correspondiente.

:::: {.question-card}
üìù **Preguntas:**

1. ¬øQu√© revelan las muestras visualizadas sobre la diversidad y complejidad del conjunto de datos?
2. ¬øQu√© tan bien representan las im√°genes sus respectivas categor√≠as?
3. ¬øQu√© desaf√≠os podr√≠an surgir al agrupar o clasificar estas im√°genes?
:::

## 2.2. Clustering y An√°lisis de Centroides
::: {.objective-card}
üéØ **Objetivo:** Aplicar clustering K-Means al conjunto de datos Fashion-MNIST y analizar los cl√∫steres resultantes.
:::

Debido a la alta dimensionalidad de los datos de imagen, agrupar directamente en valores de p√≠xeles puede ser un desaf√≠o. Sin embargo, K-Means a√∫n puede proporcionar informaci√≥n sobre la estructura del conjunto de datos. Utiliza el widget `K-Means` en Orange para agrupar las im√°genes, experimentando con diferentes valores de `K`.

Luego, visualiza los centroides de los cl√∫steres utilizando el siguiente script de Python:
```python
import matplotlib.pyplot as plt
import numpy as np

# --- CONFIGURATION ---
img_shape = (28, 28)  # adjust to your dataset

# --- LOAD DATA ---
X = in_data.X

# --- EXTRACT CLUSTERS ---
meta_names = [m.name for m in in_data.domain.metas]
if 'Cluster' in meta_names:
    clusters, _ = in_data.get_column_view('Cluster')
    clusters = np.array(clusters)
    cluster_ids = np.unique(clusters)
else:
    clusters = np.zeros(X.shape[0], dtype=int)
    cluster_ids = [0]

# --- COMPUTE CENTROIDS ---
centroids = [np.mean(X[clusters == c], axis=0) for c in cluster_ids]

# --- PLOT CENTROIDS ---
fig, axes = plt.subplots(1, len(cluster_ids), figsize=(2*len(cluster_ids), 2))
axes = np.atleast_1d(axes)
for i, ax in enumerate(axes):
    ax.imshow(centroids[i].reshape(img_shape), cmap="gray")
    ax.set_title(f"Cluster {i}")
    ax.axis("off")

plt.tight_layout()
plt.show()
```

Pega esto en un widget `Python Script` conectado a la salida de `K-Means`. Mostrar√° los centroides como im√°genes, lo que te permitir√° interpretar lo que representa cada cl√∫ster.

::: {.question-card}
üìù **Preguntas:**

1. ¬øQu√© revelan los centroides visualizados sobre las caracter√≠sticas de cada cl√∫ster?
2. ¬øC√≥mo se comparan los cl√∫steres con las categor√≠as originales en el conjunto de datos Fashion-MNIST?
3. ¬øQu√© informaci√≥n se puede extraer de los resultados del clustering en relaci√≥n con la estructura del conjunto de datos y los posibles desaf√≠os en la clasificaci√≥n?
:::

## 2.3 Representaci√≥n de Baja Dimensionalidad

::: {.objective-card}
üéØ **Objetivo:** Aprender a aplicar t√©cnicas de reducci√≥n de dimensionalidad para visualizar datos de imagen de alta dimensi√≥n.
:::

![](_resources/images/pixel.jpg){.img-right}
Los datos de alta dimensi√≥n, como las im√°genes, pueden ser dif√≠ciles de visualizar directamente. El conjunto de datos original vive en un espacio de 784 dimensiones (28√ó28), pero podemos usar la reducci√≥n de dimensionalidad para proyectar los datos en dos dimensiones mientras se preserva la mayor parte de la estructura posible.
Dos m√©todos comunes son:

- **An√°lisis de Componentes Principales (PCA):** una t√©cnica lineal que encuentra las direcciones de m√°xima varianza en los datos.
- **t-Distributed Stochastic Neighbor Embedding (t-SNE):** una t√©cnica no lineal que preserva las relaciones locales y es particularmente efectiva para visualizar cl√∫steres.

Utiliza el siguiente `Python Script` para realizar PCA en el conjunto de datos Fashion-MNIST y visualizar los resultados:
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from matplotlib.offsetbox import OffsetImage, AnnotationBbox

# --- CONFIGURATION ---
img_shape = (28, 28)

# --- LOAD DATA ---
X = in_data.X

# --- GET CLUSTER LABELS ---
meta_names = [m.name for m in in_data.domain.metas]
if 'Cluster' in meta_names:
    clusters, _ = in_data.get_column_view('Cluster')
    clusters = np.array(clusters)
else:
    clusters = np.zeros(X.shape[0], dtype=int)

cluster_labels = np.array([str(c) for c in clusters])

# --- PCA PROJECTION ---
pca = PCA(n_components=2, random_state=42)
X_2D = pca.fit_transform(X)

# --- COMPUTE CENTROIDS ---
unique_labels = np.unique(cluster_labels)
centroids = [np.mean(X[cluster_labels == lbl], axis=0) for lbl in unique_labels]
centroid_positions = [np.mean(X_2D[cluster_labels == lbl], axis=0) for lbl in unique_labels]

# --- PLOT PCA PROJECTION ---
plt.figure(figsize=(7,6))
scatter = plt.scatter(X_2D[:,0], X_2D[:,1],
                      c=np.arange(len(unique_labels))[np.searchsorted(unique_labels, cluster_labels)],
                      cmap='tab10', s=15, alpha=0.6)

handles, _ = scatter.legend_elements()
plt.legend(handles, unique_labels, title="Cluster", loc="best")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.title("PCA Projection with Cluster Centroids")

# --- ADD CENTROID IMAGES ---
for pos, centroid in zip(centroid_positions, centroids):
    imagebox = OffsetImage(centroid.reshape(img_shape), cmap='gray', zoom=0.6)
    ab = AnnotationBbox(imagebox, pos, frameon=False)
    plt.gca().add_artist(ab)

plt.tight_layout()
plt.show()
```

You can do the same with t-SNE (It might take a few minutes to compute):
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from matplotlib.offsetbox import OffsetImage, AnnotationBbox

# --- CONFIGURATION ---
img_shape = (28, 28)
perplexity = 30
random_state = 42

# --- LOAD DATA ---
X = in_data.X

# --- CLUSTER LABELS ---
meta_names = [m.name for m in in_data.domain.metas]
if 'Cluster' in meta_names:
    cluster_values, _ = in_data.get_column_view('Cluster')
    clusters = np.array(cluster_values)
else:
    clusters = np.zeros(X.shape[0], dtype=int)

cluster_labels = np.array([str(c) for c in clusters])

# --- RUN t-SNE ---
tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state, init='pca')
X_2D = tsne.fit_transform(X)

# --- COMPUTE CENTROIDS ---
unique_labels = np.unique(cluster_labels)
centroids = [np.mean(X[cluster_labels == lbl], axis=0) for lbl in unique_labels]
centroid_positions = [np.mean(X_2D[cluster_labels == lbl], axis=0) for lbl in unique_labels]

# --- PLOT t-SNE PROJECTION ---
plt.figure(figsize=(7,6))
scatter = plt.scatter(X_2D[:,0], X_2D[:,1],
                      c=np.arange(len(unique_labels))[np.searchsorted(unique_labels, cluster_labels)],
                      cmap='tab10', s=15, alpha=0.6)

handles, _ = scatter.legend_elements()
plt.legend(handles, unique_labels, title="Cluster", loc="best")
plt.xlabel("t-SNE 1")
plt.ylabel("t-SNE 2")
plt.title("t-SNE Projection with Cluster Centroids")

# --- ADD CENTROID IMAGES ---
for pos, centroid in zip(centroid_positions, centroids):
    imagebox = OffsetImage(centroid.reshape(img_shape), cmap='gray', zoom=0.7)
    ab = AnnotationBbox(imagebox, pos, frameon=False)
    plt.gca().add_artist(ab)

plt.tight_layout()
plt.show()
```

Pega cualquiera de estos en un widget de `Python Script` conectado a la salida de `K-Means`. El script proyectar√° los datos de alta dimensi√≥n en 2D y los trazar√°, coloreando los puntos seg√∫n su asignaci√≥n de cl√∫ster. Los centroides tambi√©n se muestran como im√°genes.

::: {.question-card}
üìù **Preguntas:**
1. ¬øC√≥mo difieren las proyecciones de PCA y t-SNE en t√©rminos de separaci√≥n y estructura de cl√∫steres?
2. ¬øLos cl√∫steres identificados por K-Means corresponden a regiones distintas en el espacio 2D?
3. ¬øQu√© informaci√≥n se puede obtener de las representaciones de baja dimensi√≥n sobre la estructura del conjunto de datos y los posibles desaf√≠os en la clasificaci√≥n?
:::

---

# Nota final

::: {.final-note-card}
‚ú® ¬°Felicidades por completar el Laboratorio 5! Has explorado el fascinante mundo del aprendizaje no supervisado, profundizando en t√©cnicas de clustering y reducci√≥n de dimensionalidad. Estas herramientas son invaluables para descubrir patrones ocultos en los datos, especialmente cuando las etiquetas son escasas o no est√°n disponibles.

La pr√≥xima sesi√≥n cambiaremos a un paradigma diferente: **sistemas de recomendaci√≥n**. Aqu√≠, el objetivo es predecir las preferencias del usuario bas√°ndose en el comportamiento pasado.
:::

::: {.margin}
# Explora los Laboratorios {.unnumbered .unlisted}
{{< include lab-cards.qmd >}}
:::